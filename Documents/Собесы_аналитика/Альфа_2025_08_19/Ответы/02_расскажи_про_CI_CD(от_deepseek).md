# [вариант от ChatGPT](02_расскажи_про_CI_CD(от_gpt).md) // [**вариант от DeepSeek**](02_расскажи_про_CI_CD(от_deepseek).md)

Опишу процесс CI/CD, который был настроен в одном из моих прошлых проектов (*средне-крупный микросервисный проект на Java/Spring Boot*). Это был довольно зрелый процесс, отлаженный командой.

### 1. Ветки: Develop, Test и их назначение
У нас использовалась **упрощенная** модель **`GitFlow`**.
- **Ветка `develop`**:    
    - **Назначение:** Главная ветка для интеграции новой функциональности. Это "ствол" (***trunk***), в который <u>всё сливается</u>.        
    - **Что в нее попадало:** Все завершенные `feature`-ветки после код-ревью и успешной сборки.        
    - **Состояние:** Код в `develop` _условно стабилен_. Он прошел `статический анализ` и `unit-тесты`, но не обязательно прошел полное <u>интеграционное</u> и <u>приемочное</u> тестирование. Это наша "*зона активной разработки*".        
- **Ветка `test` (или `release/*`)**:    
    - **Назначение:** Ветка для <u>стабилизации и тестирования</u> готового к выпуску функционала.        
    - **Как создавалась:** Ответственный за релиз (*Tech Lead или тимлид*) создавал ветку `test` от `develop` в преддверии очередного спринта или релиза.        
    - **Что в нее попадало:** _Только_ <u>горячие фиксы</u> (*hotfixes*) и <u>исправления багов</u>, найденных **QA** на тестовом стенде. Новая функциональность в `test` уже <u>не добавлялась</u>.        
    - **Состояние:** Код в этой ветке должен быть _готов к тестированию_ на `стейджинг`-окружении. Сборка из этой ветки разворачивалась на тестовый стенд.        

### 2. Когда собирался `Docker`-образ?
`Docker`-образ собирался **на каждом успешном билде веток `develop` и `test`** в *Jenkins*.
- **Сборка из `develop`:** После мержа пул-реквеста в `develop` дженкинс запускал пайплайн, который:    
    1. Собирал `JAR`-артефакт (*через `mvn clean package`*).        
    2. Запускал `unit-тесты`.        
    3. **Собирал Docker-образ** с тегом `latest` или с тегом, содержащим номер сборки (`build-id`) и **хеш коммита**. Этот образ пушился в приватный **Azure Container Registry (*ACR*)**.        
- **Сборка из `test`:** Процесс был аналогичным, но образ *tagged* как `staging-{version}`.   
Образ _не собирался_ на каждый коммит в `feature`-ветку, чтобы не засорять `registry`.

### 3. Как это все раскатывалось? (*Общий процесс*)
Процесс был автоматизированным и основанным на `git`-триггерах.
1. Разработчик завершает задачу в своей `feature/*` ветке.    
2. Создает **Pull Request (PR)** из `feature/*` в `develop`.    
3. Запускается **PR-билд** в *Jenkins*: <u>проверка стиля</u>, <u>сборка</u>, <u>unit-тесты</u>. Это _pre-merge_ проверка.    
4. После успешного билда и **одобрения код-ревью** минимум одним коллегой, PR мержится в `develop`.    
5. **Мерж-триггер** в `Git`-репозитории (например, в `GitHub`/`GitLab`) webhook'ом запускает соответствующий пайплайн *Jenkins* для ветки `develop`.    
6. Пайплайн для `develop` собирает артефакт, образ и деплоит его на **dev-стенд** (*автоматически*).    
7. Для деплоя на **test-стенд** процесс инициировался вручную (*см. ниже*).    
8. После тестирования на *test*-стенде, ветка `test` мержилась в `main`/`master`, создавался тег (например, `v1.2.3`), и образ с этим тегом деплоился на *production* (*часто с дополнительным **ручным** подтверждением*).    

### 4. Какой оркестратор контейнеров был применен?
**Kubernetes (*k8s*)**. Мы использовали *managed*-сервис от облачного провайдера — **Azure Kubernetes Service (*AKS*)**.
- **Причина выбора:** Мощь, гибкость, стандарт де-факто для микросервисов, богатая экосистема (Helm, Istio, многочисленные операторы).    
- **Как это работало:** У каждого сервиса был свой `helm-chart`. Пайплайн Jenkins не просто заливал образ в кластер, а **выполнял команду `helm upgrade --install`**, которая обновляла конфигурацию пода в нужном namespace (dev, staging, production) на основе значений (values) из нашего git-репозитория с чартами.    

### 5. Где был Jenkins?
Jenkins был развернут **в том же облаке (Azure), но вне Kubernetes-кластера**.
- **Архитектура:** У нас была **Master-Agent** архитектура.    
- **Jenkins Master:** Одна виртуальная машина (VM) с самим Jenkins. Он хранил состояние, управлял пайплайнами, но не выполнял тяжелые задачи.    
- **Jenkins Agents (Kubernetes Pod Templates):** Для выполнения самих jobs (сборка, деплой) Jenkins динамически создавал **поды (agents) внутри нашего AKS-кластера**. Это классический паттерн `jenkins-agent-in-k8s`.    
- **Почему так?** Экономия ресурсов (агенты создаются только на время билда и уничтожаются после) и идеальная согласованность: агент для сборки Java-приложения всегда имеет одинаковую, предсказуемую среду (образ с Maven, JDK, Docker и т.д.).    

### 6. Кто был инициатором раскатки на тестовый стенд? (Детальный механизм)
Инициатором был **Ответственный за релиз** (Tech Lead или назначенный Senior-разработчик). Механизм был **ручным, но через кнопку в Jenkins**.

**Детально:**
1. Код стабилизировался в ветке `develop`.    
2. Ответственный создавал ветку `test` от `develop`.    
3. Он заходил в Jenkins, находил Multibranch Pipeline job для своего репозитория.    
4. Переходил в подраздел для ветки `test`.    
5. **В интерфейсе сборки была кнопка "Build with Parameters"**.    
6. Нажатие на эту кнопку открывало форму, где можно было выбрать опции (например, пропустить тесты? нет; принудительно пересобрать образ? да).    
7. После нажатия "Build" запускался пайплайн для ветки `test`, который:    
    - Собирал образ с тегом `staging-{build_number}`.        
    - Пушил его в registry.        
    - Выполнял `helm upgrade ...` для namespace `staging` в нашем AKS-кластере.    
8. После успешного завершения пайплайна, он отправлял уведомление в Slack-канал команды и QA о том, что новая сборка готова к тестированию.    

### 7. Кто именно в команде писал Pipeline?
**Tech Lead и DevOps-инженер.** Они создавали и поддерживали общий **шаблон (Jenkins Shared Library)** и базовые `Jenkinsfile` для всех сервисов.
- **Разработчики** не писали пайплайны с нуля. В корне каждого репозитория лежал короткий `Jenkinsfile` (часто на 10-15 строк), который просто вызывал стандартные шаги из shared library: `buildJavaService()`, `deployToDev()`, `deployToStaging()`.    
- Это обеспечивало **стандартизацию** процесса для всех сервисов и позволяло легко вносить глобальные изменения в процесс CI/CD (например, сменить registry), правя код только в одном месте — в shared library.
    
### 8. Jenkinsfile (Стэйджи и как выглядел)
**Jenkinsfile** был декларативным. Вот его упрощенная структура:
```groovy
// Jenkinsfile в репозитории микросервиса
@pipelineLibrary('my-shared-library@main') // Подключаем общую библиотеку

pipeline {
    agent {
        kubernetes { // Запускаем весь пайплайн в динамически создаваемом агенте-поде в k8s
            label 'jenkins-agent-java'
            yamlFile 'podTemplate.yaml' // Описание пода (образы, volume'ы)
        }
    }
	
    options {
        timeout(time: 30, unit: 'MINUTES')
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }
	
    stages {
        stage('Checkout') {
            steps {
                checkout scm // Вытягиваем код из репозитория
            }
        }
		
        stage('Build & Unit Test') {
            steps {
                container('maven') { // Внутри пода есть контейнер с именем 'maven'
                    sh 'mvn clean package' // Собираем JAR
                }
            }
        }
		
        stage('SonarQube Analysis') {
            steps {
                script {
                    // Запускаем статический анализ кода
                    sonarQubeScan() // Это кастомный шаг из shared library
                }
            }
        }
		
        stage('Build Docker Image') {
            steps {
                container('docker') { // Внутри пода есть контейнер с Docker Daemon
                    script {
                        dockerBuildPush("${env.BRANCH_NAME}", "${env.BUILD_ID}")
                        // ^ Кастомный шаг из библиотеки. Собирает и пушит образ.
                    }
                }
            }
        }
		
        stage('Deploy to Dev') {
            when { branch 'develop' } // Автодеплой только для develop
            steps {
                deployToKubernetes('dev') // Шаг из библиотеки: helm upgrade...
            }
        }
		
        stage('Deploy to Staging') {
            when { branch 'test' } // Этот стэйдж активен только для ветки test
            steps {
                input message: "Deploy to STAGING?", ok: "Confirm" // Ручное подтверждение
                deployToKubernetes('staging')
            }
        }
    }
	
    post {
        always {
            cleanWs() // Всегда очищаем workspace
            script {
                // Отправляем уведомление в Slack об результате сборки
                notifySlack(currentBuild.result)
            }
        }
    }
}
```

**Ключевые стэйджи:**
1. **Checkout:** Получение кода.    
2. **Build & Unit Test:** Компиляция и первые тесты.    
3. **Static Analysis (SonarQube):** Проверка качества кода.    
4. **Build Docker Image:** Создание и публикация артефакта.    
5. **Deploy to Dev:** Автоматический деплой после мержа в `develop`.    
6. **Deploy to Staging:** _Manual-triggered_ стэйдж с подтверждением для ветки `test`.    

Такой подход делал процесс прозрачным, повторяемым и легко управляемым.

---
# [вариант от ChatGPT](02_расскажи_про_CI_CD(от_gpt).md) // [**вариант от DeepSeek**](02_расскажи_про_CI_CD(от_deepseek).md)

